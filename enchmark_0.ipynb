{
 "metadata": {
  "name": "",
  "signature": "sha256:7da371a5a4c70b60197a96aa0cc37de4bfc0aabd7c780434fd014240f5223b18"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.decomposition import TruncatedSVD\n",
      "from sklearn.svm import SVC\n",
      "# from sklearn.ensemble import RandomForestClassifier\n",
      "# from sklearn.calibration import CalibratedClassifierCV\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "from sklearn import metrics\n",
      "\n",
      "from sklearn.pipeline import Pipeline,FeatureUnion\n",
      "from sklearn import grid_search\n",
      "\n",
      "import distance\n",
      "\n",
      "\n",
      "# The following 3 functions have been taken from Ben Hamner's github repository\n",
      "# https://github.com/benhamner/Metrics\n",
      "def confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n",
      "    \"\"\"\n",
      "    Returns the confusion matrix between rater's ratings\n",
      "    \"\"\"\n",
      "    assert(len(rater_a) == len(rater_b))\n",
      "    if min_rating is None:\n",
      "        min_rating = min(rater_a + rater_b)\n",
      "    if max_rating is None:\n",
      "        max_rating = max(rater_a + rater_b)\n",
      "    num_ratings = int(max_rating - min_rating + 1)\n",
      "    conf_mat = [[0 for i in range(num_ratings)]\n",
      "                for j in range(num_ratings)]\n",
      "    for a, b in zip(rater_a, rater_b):\n",
      "        conf_mat[a - min_rating][b - min_rating] += 1\n",
      "    return conf_mat\n",
      "\n",
      "\n",
      "def histogram(ratings, min_rating=None, max_rating=None):\n",
      "    \"\"\"\n",
      "    Returns the counts of each type of rating that a rater made\n",
      "    \"\"\"\n",
      "    if min_rating is None:\n",
      "        min_rating = min(ratings)\n",
      "    if max_rating is None:\n",
      "        max_rating = max(ratings)\n",
      "    num_ratings = int(max_rating - min_rating + 1)\n",
      "    hist_ratings = [0 for x in range(num_ratings)]\n",
      "    for r in ratings:\n",
      "        hist_ratings[r - min_rating] += 1\n",
      "    return hist_ratings\n",
      "\n",
      "\n",
      "def quadratic_weighted_kappa(y, y_pred):\n",
      "    \"\"\"\n",
      "    Calculates the quadratic weighted kappa\n",
      "    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n",
      "    value, which is a measure of inter-rater agreement between two raters\n",
      "    that provide discrete numeric ratings.  Potential values range from -1\n",
      "    (representing complete disagreement) to 1 (representing complete\n",
      "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
      "    chance.\n",
      "    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
      "    each correspond to a list of integer ratings.  These lists must have the\n",
      "    same length.\n",
      "    The ratings should be integers, and it is assumed that they contain\n",
      "    the complete range of possible ratings.\n",
      "    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
      "    is the minimum possible rating, and max_rating is the maximum possible\n",
      "    rating\n",
      "    \"\"\"\n",
      "    rater_a = y\n",
      "    rater_b = y_pred\n",
      "    min_rating=None\n",
      "    max_rating=None\n",
      "    rater_a = np.array(rater_a, dtype=int)\n",
      "    rater_b = np.array(rater_b, dtype=int)\n",
      "    assert(len(rater_a) == len(rater_b))\n",
      "    if min_rating is None:\n",
      "        min_rating = min(min(rater_a), min(rater_b))\n",
      "    if max_rating is None:\n",
      "        max_rating = max(max(rater_a), max(rater_b))\n",
      "    conf_mat = confusion_matrix(rater_a, rater_b,\n",
      "                                min_rating, max_rating)\n",
      "    num_ratings = len(conf_mat)\n",
      "    num_scored_items = float(len(rater_a))\n",
      "\n",
      "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
      "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
      "\n",
      "    numerator = 0.0\n",
      "    denominator = 0.0\n",
      "\n",
      "    for i in range(num_ratings):\n",
      "        for j in range(num_ratings):\n",
      "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
      "                              / num_scored_items)\n",
      "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
      "            numerator += d * conf_mat[i][j] / num_scored_items\n",
      "            denominator += d * expected_count / num_scored_items\n",
      "\n",
      "    return (1.0 - numerator / denominator)\n",
      "\n",
      "\n",
      "def load_train(path,svd_n = 140):\n",
      "    train = pd.read_csv(path)\n",
      "    y = train.median_relevance.values    \n",
      "    train = train.drop(['id','median_relevance','median_relevance'], axis = 1)\n",
      "    \n",
      "    train_processed = do_cleaning_and_combined(train)\n",
      "    \n",
      "    tfv = TfidfVectorizer(min_df=3,  max_features=None, \n",
      "        strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n",
      "        ngram_range=(1,2), use_idf=1,smooth_idf=1,sublinear_tf=1,\n",
      "        stop_words = 'english')\n",
      "    \n",
      "    X_train = tfv.fit_transform(train_processed)\n",
      "    \n",
      "    # LSA / SVD\n",
      "    svd = None\n",
      "    if svd_n > 0:\n",
      "        svd = TruncatedSVD(n_components = svd_n) #n_components = 140\n",
      "        X_train = svd.fit_transform(X_train)\n",
      "    return train,X_train,y,tfv,svd\n",
      "    \n",
      "def load_test(path,tfv,svd):\n",
      "    test = pd.read_csv(path)\n",
      "    idx = test.id.values\n",
      "    test = test.drop(['id'], axis=1)\n",
      "    \n",
      "    test_processed = do_cleaning_and_combined(test)\n",
      "    X_test = tfv.transform(test_processed)\n",
      "    \n",
      "    if svd != None:\n",
      "        X_test = svd.transform(X_test)\n",
      "    \n",
      "    return test,X_test,idx\n",
      "\n",
      "def do_cleaning_and_combined(data):\n",
      "    processed = list(data.apply(lambda x:'%s %s %s' % (x['query'],x['product_title'], x['product_description']),axis=1))\n",
      "    return processed\n",
      "\n",
      "def feature_extraction(df):\n",
      "    \n",
      "    stop_words = [\"a\" , \"about\" , \"above\" , \"after\" , \"again\" , \"against\" , \"all\" , \"am\" , \"an\" , \"and\" , \"any\" , \"are\" , \n",
      "              \"aren't\" , \"as\" , \"at\" , \"be\" , \"because\" , \"been\" , \"before\" , \"being\" , \"below\" , \"between\" , \"both\" ,\n",
      "              \"but\" , \"by\" , \"can't\" , \"cannot\" , \"could\" , \"couldn't\" , \"did\" , \"didn't\" , \"do\" , \"does\" , \"doesn't\" , \n",
      "              \"doing\" , \"don't\" , \"down\" , \"during\" , \"each\" , \"few\" , \"for\" , \"from\" , \"further\" , \"had\" , \"hadn't\" , \n",
      "              \"has\" , \"hasn't\" , \"have\" , \"haven't\" , \"having\" , \"he\" , \"he'd\" , \"he'll\" , \"he's\" , \"her\" , \"here\" , \n",
      "              \"here's\" , \"hers\" , \"herself\" , \"him\" , \"himself\" , \"his\" , \"how\" , \"how's\" , \"i\" , \"i'd\" , \"i'll\" , \"i'm\" ,\n",
      "              \"i've\" , \"if\" , \"in\" , \"into\" , \"is\" , \"isn't\" , \"it\" , \"it's\" , \"its\" , \"itself\" , \"let's\" , \"me\" , \"more\" , \n",
      "              \"most\" , \"mustn't\" , \"my\" , \"myself\" , \"no\" , \"nor\" , \"not\" , \"of\" , \"off\" , \"on\" , \"once\" , \"only\" , \"or\" ,\n",
      "              \"other\" , \"ought\" , \"our\" , \"ours\"]\n",
      "    \n",
      "    #lower case\n",
      "    \n",
      "    df_columns = df.columns\n",
      "    df.query = df['query'].str.lower()\n",
      "    df.product_title = df.product_title.str.lower()\n",
      "    df.product_description = df.product_description.str.lower()\n",
      "    \n",
      "    # remove punchuations\n",
      "    df.query = df['query'].str.replace('[^a-zA-Z0-9\\s]', '', case=False)\n",
      "    df.product_title = df.product_title.str.replace('[^a-zA-Z0-9\\s]', '', case=False)\n",
      "    df.product_description = df.product_description.str.replace('[^a-zA-Z0-9\\s]', '', case=False)\n",
      "    \n",
      "    #remove new line characters\n",
      "    df.product_description = df.product_description.str.replace('\\n', ' ', case=False)\n",
      "    \n",
      "    #removing 'es' or 's' at the end of the words\n",
      "    df.query = df['query'].str.replace('es\\\\b|s\\\\b', '', case=False)\n",
      "    df.product_title = df.product_title.str.replace('es\\\\b|s\\\\b', '', case=False)\n",
      "    df.product_description = df.product_description.str.replace('es\\\\b|s\\\\b', '', case=False)\n",
      "    \n",
      "    #remove colors\n",
      "    df.product_title = df.product_title.str.replace('red|blue|pink|black|yellow', '', case=False)\n",
      "    \n",
      "    df['query_length'] = df.apply(lambda x : len(x.query.split()), axis = 1)\n",
      "    df['title_length'] = df.apply(lambda x : len(x.product_title.split()), axis = 1)\n",
      "    df['desc_length'] = df.apply(lambda x : len(str(x.product_description).split()), axis = 1)\n",
      "    \n",
      "#     df['desc_length_log'] = np.log(df['desc_length'])\n",
      "    \n",
      "    df['query_title_matched_words'] = df.apply(lambda x: len(set(x.query.split()).intersection(set(x.product_title.split()))), axis = 1)\n",
      "    \n",
      "    df['jacard_dist'] = df.apply(lambda x: distance.jaccard(x.query,x.product_title), axis = 1)\n",
      "    df['jacard_dist_words'] = df.apply(lambda x: distance.jaccard(x.query.split(),x.product_title.split()), axis = 1)\n",
      "    \n",
      "    df['query_1st_word_in_title'] = df.apply(lambda x: x.product_title.split().count(x.query.split()[0]), axis = 1)\n",
      "    \n",
      "    print df.shape\n",
      "    df = df.drop(df_columns,axis = 1)\n",
      "    return df\n",
      "\n",
      "\n",
      "\n",
      "train,X_train,y,tfv,svd = load_train(path=\"data/train.csv\",svd_n = 0)\n",
      "test,X_test,idx = load_test(\"data/test.csv\",tfv,svd)\n",
      "\n",
      "# print 'X_train shape',X_train.shape\n",
      "# train_features = feature_extraction(train)\n",
      "# test_features = feature_extraction(test)\n",
      "\n",
      "# print 'Extracted features shape',train_features.shape\n",
      "\n",
      "# X_train = np.hstack((X_train.toarray(), train_features.values))\n",
      "# X_test = np.hstack((X_test.toarray(), test_features.values))\n",
      " \n",
      "    \n",
      "# Initialize SVD\n",
      "svd = TruncatedSVD()\n",
      "    \n",
      "# Initialize the standard scaler \n",
      "scl = StandardScaler()\n",
      "    \n",
      "# We will use SVM here..\n",
      "svm_model = SVC()\n",
      "    \n",
      "# Create the pipeline \n",
      "\n",
      "clf = Pipeline([('svd', svd),\n",
      "                         ('scl', scl),\n",
      "                         ('svm', svm_model)])\n",
      "    \n",
      "# Create a parameter grid to search for best parameters for everything in the pipeline\n",
      "param_grid = {'svd__n_components' : [100,150,200,250,300,350,400],\n",
      "                  'svm__C': [1,10,100,1000,1.e4,1.e5]}\n",
      "    \n",
      "# Kappa Scorer \n",
      "kappa_scorer = metrics.make_scorer(quadratic_weighted_kappa, greater_is_better = True)\n",
      "    \n",
      "# Initialize Grid Search Model\n",
      "model = grid_search.GridSearchCV(estimator = clf, param_grid=param_grid, scoring=kappa_scorer,\n",
      "                                     verbose=10, n_jobs=-1, iid=True, refit=True, cv=4)\n",
      "                                     \n",
      "# Fit Grid Search Model\n",
      "model.fit(X_train, y)\n",
      "print(\"Best score: %0.3f\" % model.best_score_)\n",
      "print(\"Best parameters set:\")\n",
      "best_parameters = model.best_estimator_.get_params()\n",
      "for param_name in sorted(param_grid.keys()):\n",
      "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
      "    \n",
      "# Get best model\n",
      "best_model = model.best_estimator_\n",
      "    \n",
      "# Fit best SVM Model\n",
      "best_model.fit(X_train, y)\n",
      "preds = best_model.predict(X_test)\n",
      "\n",
      "# Create submission file\n",
      "submission = pd.DataFrame({\"id\": idx, \"prediction\": preds})\n",
      "submission.to_csv(\"predictions/benchmark_1_with_cv.csv\", index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Fitting 4 folds for each of 42 candidates, totalling 168 fits\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "[Parallel(n_jobs=-1)]: Done   1 jobs       | elapsed:   43.4s\n",
        "[Parallel(n_jobs=-1)]: Done   2 jobs       | elapsed:   43.8s\n",
        "[Parallel(n_jobs=-1)]: Done   5 jobs       | elapsed:  1.4min\n",
        "[Parallel(n_jobs=-1)]: Done   8 jobs       | elapsed:  1.5min\n",
        "[Parallel(n_jobs=-1)]: Done  13 jobs       | elapsed:  3.1min\n",
        "[Parallel(n_jobs=-1)]: Done  18 jobs       | elapsed:  4.0min\n",
        "[Parallel(n_jobs=-1)]: Done  25 jobs       | elapsed:  5.8min\n",
        "[Parallel(n_jobs=-1)]: Done  32 jobs       | elapsed:  7.3min\n",
        "[Parallel(n_jobs=-1)]: Done  41 jobs       | elapsed: 10.4min\n",
        "[Parallel(n_jobs=-1)]: Done  50 jobs       | elapsed: 13.2min\n",
        "[Parallel(n_jobs=-1)]: Done  61 jobs       | elapsed: 17.7min\n",
        "[Parallel(n_jobs=-1)]: Done  72 jobs       | elapsed: 21.6min\n",
        "[Parallel(n_jobs=-1)]: Done  85 jobs       | elapsed: 27.9min\n",
        "[Parallel(n_jobs=-1)]: Done  98 jobs       | elapsed: 34.1min\n",
        "[Parallel(n_jobs=-1)]: Done 113 jobs       | elapsed: 43.3min\n",
        "[Parallel(n_jobs=-1)]: Done 128 jobs       | elapsed: 52.9min\n",
        "[Parallel(n_jobs=-1)]: Done 145 jobs       | elapsed: 67.2min\n",
        "[Parallel(n_jobs=-1)]: Done 162 out of 168 | elapsed: 82.8min remaining:  3.1min\n",
        "[Parallel(n_jobs=-1)]: Done 168 out of 168 | elapsed: 86.6min finished\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Best score: 0.566\n",
        "Best parameters set:\n",
        "\tsvd__n_components: 200\n",
        "\tsvm__C: 10\n",
        "[CV] svm__C=1, svd__n_components=100 .................................\n",
        "[CV] svm__C=1, svd__n_components=100 .................................\n",
        "[CV] svm__C=1, svd__n_components=100 .................................\n",
        "[CV] svm__C=1, svd__n_components=100 .................................\n",
        "[CV] ........ svm__C=1, svd__n_components=100, score=0.079535 -  43.3s[CV] ........ svm__C=1, svd__n_components=100, score=0.074764 -  43.7s[CV] ........ svm__C=1, svd__n_components=100, score=0.077406 -  43.5s[CV] ........ svm__C=1, svd__n_components=100, score=0.089227 -  44.8s\n",
        "\n",
        "\n",
        "\n",
        "[CV] svm__C=10, svd__n_components=100 ................................[CV] svm__C=10, svd__n_components=100 ................................[CV] svm__C=10, svd__n_components=100 ................................[CV] svm__C=10, svd__n_components=100 ................................\n",
        "\n",
        "\n",
        "\n",
        "[CV] ....... svm__C=10, svd__n_components=100, score=0.491599 -  43.0s[CV] ....... svm__C=10, svd__n_components=100, score=0.468533 -  43.7s[CV] ....... svm__C=10, svd__n_components=100, score=0.460444 -  44.2s[CV] ....... svm__C=10, svd__n_components=100, score=0.412620 -  43.7s\n",
        "\n",
        "\n",
        "\n",
        "[CV] svm__C=100, svd__n_components=100 ...............................[CV] svm__C=100, svd__n_components=100 ...............................\n",
        "[CV] svm__C=100, svd__n_components=100 ...............................[CV] svm__C=100, svd__n_components=100 ...............................\n",
        "\n",
        "[CV] ...... svm__C=100, svd__n_components=100, score=0.477542 -  54.3s\n",
        "[CV] ...... svm__C=100, svd__n_components=100, score=0.442733 -  55.9s[CV] ...... svm__C=100, svd__n_components=100, score=0.466295 -  53.3s\n",
        "[CV] ...... svm__C=100, svd__n_components=100, score=0.458299 -  49.3s\n",
        "\n",
        "[CV] svm__C=1000, svd__n_components=100 ..............................\n",
        "[CV] svm__C=1000, svd__n_components=100 ..............................\n",
        "[CV] svm__C=1000, svd__n_components=100 ..............................\n",
        "\n",
        "[CV] svm__C=1000, svd__n_components=100 ..............................\n",
        "[CV] ..... svm__C=1000, svd__n_components=100, score=0.416623 -  55.3s[CV] ..... svm__C=1000, svd__n_components=100, score=0.434515 -  50.7s[CV] ..... svm__C=1000, svd__n_components=100, score=0.462844 -  54.7s[CV] ..... svm__C=1000, svd__n_components=100, score=0.460630 -  50.8s\n",
        "\n",
        "\n",
        "\n",
        "[CV] svm__C=10000.0, svd__n_components=100 ...........................[CV] svm__C=10000.0, svd__n_components=100 ...........................[CV] svm__C=10000.0, svd__n_components=100 ...........................[CV] svm__C=10000.0, svd__n_components=100 ...........................\n",
        "\n",
        "\n",
        "\n",
        "[CV] .. svm__C=10000.0, svd__n_components=100, score=0.434053 -  54.9s[CV] .. svm__C=10000.0, svd__n_components=100, score=0.422978 -  51.8s[CV] .. svm__C=10000.0, svd__n_components=100, score=0.445849 -  44.6s[CV] .. svm__C=10000.0, svd__n_components=100, score=0.447951 -  53.8s\n",
        "\n",
        "\n",
        "\n",
        "[CV] svm__C=100000.0, svd__n_components=100 ..........................\n",
        "[CV] svm__C=100000.0, svd__n_components=100 ..........................\n",
        "[CV] svm__C=100000.0, svd__n_components=100 ..........................\n",
        "[CV] svm__C=100000.0, svd__n_components=100 ..........................\n",
        "[CV] . svm__C=100000.0, svd__n_components=100, score=0.406926 -  56.3s[CV] . svm__C=100000.0, svd__n_components=100, score=0.445363 -  46.5s[CV] . svm__C=100000.0, svd__n_components=100, score=0.411533 -  47.6s[CV] . svm__C=100000.0, svd__n_components=100, score=0.401333 -  47.0s\n",
        "\n",
        "\n",
        "\n",
        "[CV] svm__C=1, svd__n_components=150 .................................[CV] svm__C=1, svd__n_components=150 .................................[CV] svm__C=1, svd__n_components=150 .................................\n",
        "[CV] svm__C=1, svd__n_components=150 .................................\n",
        "\n",
        "\n",
        "[CV] ........ svm__C=1, svd__n_components=150, score=0.183151 - 1.1min[CV] ........ svm__C=1, svd__n_components=150, score=0.226460 - 1.0min[CV] ........ svm__C=1, svd__n_components=150, score=0.174919 - 1.0min[CV] ........ svm__C=1, svd__n_components=150, score=0.243503 -  59.2s\n",
        "\n",
        "\n",
        "\n",
        "[CV] svm__C=10, svd__n_components=150 ................................\n",
        "[CV] svm__C=10, svd__n_components=150 ................................\n",
        "[CV] svm__C=10, svd__n_components=150 ................................\n",
        "[CV] svm__C=10, svd__n_components=150 ................................\n",
        "[CV] ....... svm__C=10, svd__n_components=150, score=0.537647 - 1.1min[CV] ....... svm__C=10, svd__n_components=150, score=0.550550 - 1.1min[CV] ....... svm__C=10, svd__n_components=150, score=0.528322 - 1.1min[CV] ....... svm__C=10, svd__n_components=150, score=0.540613 - 1.0min\n",
        "\n",
        "\n",
        "\n",
        "[CV] svm__C=100, svd__n_components=150 ...............................[CV] svm__C=100, svd__n_components=150 ...............................\n",
        "[CV] svm__C=100, svd__n_components=150 ...............................\n",
        "[CV] svm__C=100, svd__n_components=150 ...............................\n",
        "\n",
        "[CV] ...... svm__C=100, svd__n_components=150, score=0.493002 - 1.3min[CV] ...... svm__C=100, svd__n_components=150, score=0.505854 - 1.3min[CV] ...... svm__C=100, svd__n_components=150, score=0.505794 - 1.2min[CV] ...... svm__C=100, svd__n_components=150, score=0.516238 - 1.3min\n",
        "\n",
        "\n",
        "\n",
        "[CV] svm__C=1000, svd__n_components=150 ..............................[CV] svm__C=1000, svd__n_components=150 ..............................\n",
        "[CV] svm__C=1000, svd__n_components=150 ..............................\n",
        "[CV] svm__C=1000, svd__n_components=150 ..............................\n",
        "\n",
        "[CV] ..... svm__C=1000, svd__n_components=150, score=0.503141 - 1.2min[CV] ..... svm__C=1000, svd__n_components=150, score=0.498250 - 1.1min[CV] ..... svm__C=1000, svd__n_components=150, score=0.482017 - 1.3min[CV] ..... svm__C=1000, svd__n_components=150, score=0.485590 - 1.3min\n",
        "\n",
        "\n",
        "\n",
        "[CV] svm__C=10000.0, svd__n_components=150 ...........................[CV] svm__C=10000.0, svd__n_components=150 ...........................\n",
        "[CV] svm__C=10000.0, svd__n_components=150 ...........................\n",
        "[CV] svm__C=10000.0, svd__n_components=150 ...........................\n",
        "\n",
        "[CV] .. svm__C=10000.0, svd__n_components=150, score=0.515010 - 1.2min[CV] .. svm__C=10000.0, svd__n_components=150, score=0.465807 - 1.2min[CV] .. svm__C=10000.0, svd__n_components=150, score=0.490238 - 1.2min[CV] .. svm__C=10000.0, svd__n_components=150, score=0.483751 - 1.3min\n",
        "\n",
        "\n",
        "\n",
        "[CV] svm__C=100000.0, svd__n_components=150 ..........................[CV] svm__C=100000.0, svd__n_components=150 ..........................[CV] svm__C=100000.0, svd__n_components=150 ..........................[CV] svm__C=100000.0, svd__n_components=150 ..........................\n",
        "\n",
        "\n",
        "\n",
        "[CV] . svm__C=100000.0, svd__n_components=150, score=0.449349 - 1.3min[CV] . svm__C=100000.0, svd__n_components=150, score=0.497195 - 1.3min[CV] . svm__C=100000.0, svd__n_components=150, score=0.448781 - 1.3min[CV] . svm__C=100000.0, svd__n_components=150, score=0.482976 - 1.2min\n",
        "\n",
        "\n",
        "\n",
        "[CV] svm__C=1, svd__n_components=200 .................................[CV] svm__C=1, svd__n_components=200 .................................[CV] svm__C=1, svd__n_components=200 .................................\n",
        "[CV] svm__C=1, svd__n_components=200 .................................\n",
        "\n",
        "[CV] ........ svm__C=1, svd__n_components=200, score=0.293414 - 1.3min\n",
        "[CV] ........ svm__C=1, svd__n_components=200, score=0.304967 - 1.3min[CV] ........ svm__C=1, svd__n_components=200, score=0.321741 - 1.3min\n",
        "[CV] ........ svm__C=1, svd__n_components=200, score=0.293837 - 1.3min\n",
        "\n",
        "[CV] svm__C=10, svd__n_components=200 ................................\n",
        "[CV] svm__C=10, svd__n_components=200 ................................\n",
        "[CV] svm__C=10, svd__n_components=200 ................................\n",
        "[CV] svm__C=10, svd__n_components=200 ................................[CV] ....... svm__C=10, svd__n_components=200, score=0.576220 - 1.4min\n",
        "[CV] ....... svm__C=10, svd__n_components=200, score=0.548083 - 1.4min\n",
        "\n",
        "[CV] ....... svm__C=10, svd__n_components=200, score=0.568048 - 1.4min\n",
        "[CV] ....... svm__C=10, svd__n_components=200, score=0.571663 - 1.3min[CV] svm__C=100, svd__n_components=200 ...............................\n",
        "[CV] svm__C=100, svd__n_components=200 ...............................\n",
        "\n",
        "\n",
        "[CV] svm__C=100, svd__n_components=200 ...............................[CV] ...... svm__C=100, svd__n_components=200, score=0.518453 - 1.5min[CV] svm__C=100, svd__n_components=200 ...............................[CV] ...... svm__C=100, svd__n_components=200, score=0.525772 - 1.6min\n",
        "\n",
        "\n",
        "\n",
        "[CV] ...... svm__C=100, svd__n_components=200, score=0.539358 - 1.6min[CV] svm__C=1000, svd__n_components=200 ..............................[CV] ...... svm__C=100, svd__n_components=200, score=0.507761 - 1.5min[CV] svm__C=1000, svd__n_components=200 ..............................\n",
        "\n",
        "\n",
        "\n",
        "[CV] svm__C=1000, svd__n_components=200 ..............................\n",
        "[CV] ..... svm__C=1000, svd__n_components=200, score=0.512583 - 1.7min[CV] svm__C=1000, svd__n_components=200 ..............................[CV] ..... svm__C=1000, svd__n_components=200, score=0.493692 - 1.7min[CV] ..... svm__C=1000, svd__n_components=200, score=0.521049 - 1.7min\n",
        "\n",
        "\n",
        "\n",
        "[CV] svm__C=10000.0, svd__n_components=200 ...........................\n",
        "[CV] ..... svm__C=1000, svd__n_components=200, score=0.519502 - 1.5min[CV] svm__C=10000.0, svd__n_components=200 ...........................\n",
        "[CV] svm__C=10000.0, svd__n_components=200 ...........................[CV] .. svm__C=10000.0, svd__n_components=200, score=0.511822 - 1.5min\n",
        "[CV] .. svm__C=10000.0, svd__n_components=200, score=0.487131 - 1.6min\n",
        "\n",
        "[CV] svm__C=10000.0, svd__n_components=200 ...........................\n",
        "\n",
        "[CV] .. svm__C=10000.0, svd__n_components=200, score=0.509849 - 1.6min[CV] svm__C=100000.0, svd__n_components=200 ..........................\n",
        "[CV] .. svm__C=10000.0, svd__n_components=200, score=0.511894 - 1.5min[CV] svm__C=100000.0, svd__n_components=200 ..........................\n",
        "\n",
        "[CV] . svm__C=100000.0, svd__n_components=200, score=0.496586 - 1.6min\n",
        "[CV] . svm__C=100000.0, svd__n_components=200, score=0.486855 - 1.6min[CV] svm__C=100000.0, svd__n_components=200 ..........................\n",
        "\n",
        "[CV] svm__C=100000.0, svd__n_components=200 ..........................\n",
        "[CV] . svm__C=100000.0, svd__n_components=200, score=0.498701 - 1.6min[CV] svm__C=1, svd__n_components=250 .................................\n",
        "\n",
        "[CV] svm__C=1, svd__n_components=250 .................................\n",
        "[CV] ........ svm__C=1, svd__n_components=250, score=0.308349 - 1.7min[CV] . svm__C=100000.0, svd__n_components=200, score=0.502573 - 1.6min\n",
        "[CV] svm__C=1, svd__n_components=250 .................................\n",
        "\n",
        "\n",
        "[CV] ........ svm__C=1, svd__n_components=250, score=0.345947 - 1.6min"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## LB Score : 0.55447"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}
